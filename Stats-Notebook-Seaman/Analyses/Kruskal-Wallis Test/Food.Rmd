---
title: "Food at College"
subtitle: "MATH 325"
author: "Andrew Seaman"
date: 2025-Jun-27
output: 
  html_document:
    theme: journal
    toc: true
    toc_float: false #true
    toc-level: 3
    highlight: pygment
    code_folding: hide
---

```{=html}
<!--******************************************************
LAST UPDATED: 2025-07-06
BY: Andrew Seaman
*******************************************************-->
```

<!-- Today's date -->

```{r, warning=FALSE, echo=FALSE}
# Utility: copy today's date to clipboard, only when editing
if (interactive()) {
  clipr::write_clip(Sys.Date()) # Date format: 'yyyy-mm-dd'
}

# Next, paste after 'LAST UPDATED' above
```

```{=html}
<!--******************************************************
Code Table of Contents (Code Toc)
*******************************************************-->
```

<!-- Create: Code TOC -->

```{r, warning=FALSE, echo=FALSE, message=FALSE}
# ————————————————————————————————————————————————————————————————————————————
#              Create the 'Code Table of Contents' (Code Toc)
# ————————————————————————————————————————————————————————————————————————————
# Libraries
library(pander)

# Lists (****MANUAL EDITING REQUIRED****)
sections <- c("SETUP", "INSTRUCTIONS", "BACKGROUND", "EXPLORATION", "ANALYSIS")
start_line <- c(92, 166, 594, 687, 875) # Rough start of section
end_line <- c(155, 568, 576, 864, 1113)    # Rough end of section

# Dataframe
code_toc <- data.frame(
  Section = sections,
  Start = start_line,
  End = end_line
)

# Markdown formatted dataframe
formatted_code_toc <- pander(code_toc)

# Copy markdown formatted dataframe (paste below)
if (interactive()) {
  clipr::write_clip(formatted_code_toc)
}
```

```{=html}
<!-- 
Display: Code TOC
Updated: 2025-07-06

-----------------------------
   Section      Start   End  
-------------- ------- ------
    SETUP        92     155  

 INSTRUCTIONS    166    568  

  BACKGROUND     594    576  

 EXPLORATION     687    864  

   ANALYSIS      875    1113 
-----------------------------
-->
```

```{=html}
<!--
.
.
.
.
.
.
-->
```

```{=html}
<!-------------------------------------------------------
.                          SETUP                    
-------------------------------------------------------->
```

## Setup {.tabset .tabset-fade}

### Hide

*Click the tabs to read details and see the code.*

----

</BR>

<!-- Libraries -->

### Libraries

These libraries provide the tools necessary for this analysis by way of data sourcing, manipulation, and visualization (see chunk).

```{r, warning=FALSE, message=FALSE}
library(mosaic)
library(car)
library(DT)
library(pander)
library(readr)
library(tidyverse) # Includes the 'tidyr' package
```

<!-- Format HTML Output -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

----

</BR>

### Data

The data source is called __Food__ (see _Background_ for details).

```{r, warning=FALSE, message=FALSE}
# —————————————
# INSTRUCTIONS:
# —————————————

# From your menu bar (at the top of R-Studio) select:
#   1) "Session -> Set working directory -> To source file location"
#   2) Then play this chunk to get the data into R.


# Data import: source file variables for later construction
file_name <- "food.csv"
file_path <- "../../Data"

# Data import: construct the full file path and import the data
food <- read_csv(file.path(file_path, file_name)) 

# Note: I used this creative method to import the data (but definitely not neccessary in this context. Kind of clear though).
```

----

</BR>

### Comments to Critiquers

If you want to give your critiquers some ideas about what you have <u>questions</u> on and would like <u>help</u> with, place those details here.

Notes:

1.  I included a section called **"Exploration"** which shows how I navigated the data to determine my research question.
2.  The plots at the end of my analysis are more there because I was curious and have less value towards the ultimate goal of this analysis than do the earlier plots.

<!--End of comments -->

----

</BR>

```{=html}
<!--
.
.
.
.
.
.
-->
```

```{=html}
<!-------------------------------------------------------
.                          INSTRUCTIONS                    
-------------------------------------------------------->
```

```{=html}
<!--
## Instructions {.tabset .tabset-fade}

### Hide

_Click the tabs to view the instructions._

### Show

<span style="font-weight: bold; background-color: yellow;">Use the 'Food' dataset and a Kruskal-Wallis Test(s) to answer an interesting question(s) that you come up with.</span>
 
 About the dataset:
 
  * GPA - numerical, self-reported current college GPA
  * weight - numerical, self-reported weight in lbs
  * gender - "Female", "Male"
  * breakfast - "Cereal", "Donut" 
  
      The participants were shown a picture of "Cereal" and a "Donut" and asked which one of these pictures they associate with the word "breakfast."


  * calories_chicken - guessing calories in chicken piadina from the options of:
     1 - 265 
     2 - 430 
     3 - 610 
     4 - 720
    (the variable shows the actual number of calories participants selected)


   * calories_day - Importance of consuming calories per day
     1 - i dont know how many calories i should consume
     2 - it is not at all important 
     3 - it is moderately important 
     4 - it is very important


   * comfort_food - List 3-5 comfort foods that come to mind. 
     Open ended 


   * comfort_food_reasons - What are some of the reasons that make you eat comfort food?      (i.e., anger, sadness, happiness, boredom, etc) - list up to three
     Open ended


   * comfort_food_reasons_coded
     1 - stress
     2 - boredom
     3 - depression/sadness
     4 - hunger
     5 - laziness
     6 - cold weather
     7 - happiness 
     8 - watching tv
     9 - none 


    * cook - how often do you cook?
     1 - Every day 
     2 - A couple of times a week 
     3 - Whenever I can, but that is not very often  
     4 - I only help a little during holidays 
     5 - Never, I really do not know my way around a kitchen


    * cuisine - what type of cuisine did you eat growing up?
     1 - American
     2 - Mexican.Spanish
     3 - Korean/Asian
     4 - Indian
     5 - American inspired international dishes
     6 - other


    * diet_current - describe your current diet
     open ended

    
    * diet_current_coded
     1 - healthy/balanced/moderated/
     2 - unhealthy/cheap/too much/random/
     3 - the same thing over and over
     4 - unclear


    * drink - which picture do you associate with the word “drink”?
     1 - orange juice
     2 - soda 


    * eating_changes  - Describe your eating changes since the moment you got into college?
     Open ended 


    * eating_changes_coded
     1 - worse
     2 - better
     3 - the same
     4 - unclear


    * eating_changes_coded1
     1 - eat faster
     2 - bigger quantity
     3 - worse quality 
     4 - same food
     5 - healthier
     6 - unclear
     7 - drink coffee 
     8 - less food
     9 - more sweets
     10 - timing 
     11 - more carbs or snacking
     12 - drink more water
     13 - more variety


    * eating_out - frequency of eating out in a typical week 
     1 - Never 
     2 - 1-2 times 
     3 - 2-3 times 
     4 - 3-5 times 
     5 - every day


    * employment - do you work? 
     1 - yes full time 
     2 - yes part time 
     3 - no
     4  - other


    * ethnic_food - How likely to eat ethnic food 
     1 - very unlikely 
     2 - unlikely 
     3 - neutral 
     4 - likely 
     5 - very likely 


    * exercise - how often do you exercise in a regular week?
     1 - Everyday 
     2 - Twice or three times per week 
     3 - Once a week
     4 - Sometimes 
     5 - Never


    * father_education - 
     1 - less than high school 
     2 - high school degree 
     3 - some college degree 
     4 - college degree 
     5 - graduate degree 


    * father_profession - what is your father profession?
     Open ended


    * fav_cuisine - What is your favorite cuisine?
     Open ended


    * fav_cuisine_coded
     0-none
     1 - Italian/French/greek
     2 - Spanish/mexican
     3 - Arabic/Turkish
     4 - asian/chineses/thai/nepal
     5 - American
     6 - African 
     7 - Jamaican
     8 - indian


    * fav_food - was your favorite food cooked at home or store bought? 
     1 - cooked at home 
     2 - store bought 
     3 - both bought at store and cooked at home


    * food_childhood - what was your favorite childhood food?
     Open ended



    * fries - which of these pictures you associate with word fries? 
     1 - Mcdonald’s fries
     2 - home fries

 
    * fruit_day - How likely to eat fruit in a regular day 
     1 - very unlikely 
     2 - unlikely 
     3 - neutral 
     4 - likely 
     5 - very likely 


    * grade_level – 
     1 - freshman 
     2 -Sophomore 
     3 - Junior 
     4 - Senior
     
      
    * greek_food - How likely to eat greek food when available?
     1 - very unlikely 
     2 - unlikely 
     3 - neutral 
     4 - likely 
     5 - very likely 
     
      
    * healthy_feel – how likely are you to agree with the following statement: “I feel very healthy!” ?
      1 to 10 where 1 is strongly agree and 10 is strongly disagree - scale
      
      
    * healthy_meal – what is a healthy meal? Describe in 2-3 sentences.
      Open ended
      
      
    * ideal_diet – describe your ideal diet in 2-3 sentences
      Open ended
      
      
    * Ideal_diet_coded
      1 – portion control
      2 – adding veggies/eating healthier food/adding fruit 
      3 – balance
      4 – less sugar
      5 – home cooked/organic
      6 – current diet
      7 – more protein
      8 – unclear
      
      
     * income
      1 - less than $15,000 
      2 - $15,001 to $30,000 
      3 - $30,001 to $50,000 
      4 - $50,001 to $70,000 
      5 - $70,001 to $100,000 
      6 - higher than $100,000
      
     * indian_food – how likely are you to eat indian food when available
      1 - very unlikely 
      2 - unlikely 
      3 - neutral 
      4 - likely 
      5 - very likely 
      
      
     * Italian_food – how likely are you to eat Italian food when available?
      1 - very unlikely 
      2 - unlikely 
      3 - neutral 
      4 - likely 
      5 - very likely 
      
      
     * life_rewarding – how likely are you to agree with the following statement: “I feel life is very rewarding!” ?
      1 to 10 where 1 is strongly agree and 10 is strongly disagree - scale
      
      
     * marital_status
      1 -Single 
      2 - In a relationship 
      3 - Cohabiting 
      4 - Married 
      5 - Divorced 
      6 - Widowed
      
      
     * meals_dinner_friend – What would you serve to a friend for dinner?
      Open ended
      
      
     * mothers_education 
      1 - less than high school 
      2 - high school degree 
      3 - some college degree 
      4 - college degree 
      5 - graduate degree
      
      
     * mothers_profession – what is your mother’s profession? 
      
      
     * nutritional_check - checking nutritional values frequency 
      1 - never 
      2 - on certain products only 
      3 - very rarely 
      4 - on most products 
      5 - on everything
      
      
     * on_off_campus – living situation
      1 - On campus 
      2 - Rent out of campus 
      3 - Live with my parents and commute 
      4 - Own my own house
      
      
     * parents_cook - Approximately how many days a week did your parents cook? 
      1 - Almost everyday 
      2 - 2-3 times a week 
      3 - 1-2 times a week 
      4 - on holidays only 
      5 - never
      
      
     * pay_meal_out - How much would you pay for meal out? 
      1 - up to $5.00 
      2 - $5.01 to $10.00 
      3 - $10.01 to $20.00 
      4 - $20.01 to $30.00 
      5 - $30.01 to $40.00 
      6 - more than $40.01 
      
      
     * Persian_food - How likely to eat persian food when available?
      1 - very unlikely 
      2 - unlikely 
      3 - neutral 
      4 - likely 
      5 - very likely 
      
      
     * self_perception_weight - self perception of weight 
      6 - i dont think myself in these terms 
      5 - overweight 
      4 - slightly overweight 
      3 - just right 
      2 - very fit 
      1 - slim 
      
      
     * Which of the two pictures you associate with the word soup?
      1 – veggie soup
      2 – creamy soup
      
      
     * sports - sports – do you do any sporting activity?
      1 - Yes 
      2 - No 
      99 – no answer
      
      
     * thai_food - How likely to eat thai food when available?
      1 - very unlikely 
      2 - unlikely 
      3 - neutral 
      4 - likely 
      5 - very likely 
      
      
     * tortilla_calories - guessing calories in a burrito sandwhich from Chipolte?
      1 - 580 
      2 - 725 
      3 - 940 
      4 - 1165
      
     * turkey_calories - Can you guess how many calories are in the foods shown below?
     (Panera Bread Roasted Turkey and Avocado BLT)
      1 - 345 
      2 - 500 
      3 - 690 
      4 - 850
      
      
     * type_sports – what type of sports are you involved?
      Open-ended
      
     * veggies_day - How likely to eat veggies in a day? 
      1 - very unlikely 
      2 - unlikely 
      3 - neutral 
      4- likely 
      5 - very likely
      
     * vitamins – do you take any supplements or vitamins?
      1 – yes
      2 – no
      
     * waffle_calories - guessing calories in waffle potato sandwhich 
      1 - 575 
      2 - 760 
      3 - 900 
      4 - 1315

Note this dataset and description come from: © 2020 Kaggle Inc for further details visit:

[Food choices on Kaggle](https://www.kaggle.com/borapajo/food-choices?select=food_coded.csv)

Food choices and preferences of college students
This dataset includes information on food choices, nutrition, preferences, childhood favorites, and other information from college students. There are 126 responses from students. Data is raw and uncleaned. Cleaning is in the process and as soon as that is done, additional versions of the data will be posted.

</BR>

-->
```

```{=html}
<!--
.
.
.
.
.
.
-->
```

```{=html}
<!--*****************************************************
.                          BEGIN ANALYSIS                    
******************************************************-->
```

```{=html}
<!--
.
.
.
.
.
.
-->
```

```{=html}
<!-------------------------------------------------------
.                          BACKGROUND                    
-------------------------------------------------------->
```

## Background {.tabset .tabset-fade}

<!-- NOTE: 
I don't think I need to have a 'Hide' option in this tabset. I want the background to be shown by default. -->

<!-- ### Hide -->

<!-- *Click the tab to view details and read code.* -->

### Description

#### Food choices and preferences of college students

<!-- From the website linked below -->

This dataset includes information on food choices, nutrition, preferences, childhood favorites, and other information from college students. There are 126 responses from students. Data is raw and uncleaned. Cleaning is in the process and as soon as that is done, additional versions of the data will be posted.

<!-- Note on the data -->

Note this dataset and description come from: © 2020 Kaggle Inc

For further details visit: [Food choices on Kaggle](https://www.kaggle.com/borapajo/food-choices?select=food_coded.csv)

----

</BR>

### Data Summary {.tabset .tabset-fade}

The **Food** dataset has over 50 columns of data. Click through the tabs below to see summarizations and detail. (Code source: ChatGPT)

#### Hide

*Click the buttons to view the summaries.*

----

</BR>

#### 1. Structure

```{r}
str(food)
```

----

</BR>

#### 2. Head

```{r}
head(food)
```

----

</BR>

#### 3. Summary (numericals)

```{r}
summary(food)
```

----

</BR>

#### 4. Summary (Categoricals)

<!-- Levels & Counts -->

```{r}
# Instantiate empty list for variables with factor levels
factor_vars <- c()

# Find columns
for (colname in names(food)) {
  column <- food[[colname]] # grab the column itself
  
  if (is.factor(column)) {
    # store column names
    factor_vars <- c(factor_vars, colname)
  }
}

# Show table for each variable
for (colname in factor_vars) {
  table(food$colname)
}
```

----

</BR>

#### 5. Count NA

```{r}
colSums(is.na(food))
```

----

</BR>

#### 6. All Vars

```{r}
names(food)
```

----

</BR>

#### 7. Count row/col

```{r}
nrow(food)
ncol(food)
```

----

</BR>

```{=html}
<!--
.
.
.
.
.
.
-->
```

```{=html}
<!-------------------------------------------------------
.                          EXPLORATION                    
-------------------------------------------------------->
```

## Exploration {.tabset .tabset-fade}

### Hide

*Click the tabs to view details and read code.*

----

</BR>

<!-- This secondary tabset is used for the tables below and to contain the code chunks that have no output. -->

### Variable Categorization {.tabset .tabset-pills .tabset-fade}

The following variables are up for consideration in this analysis which aims to __assess__ the presence of __variation__ among nonparametric groups.

First, we algorithmicaly categorize all of the variables from the dataset.

```{r}
# Sort variables by type
numerical_vars <- c()
categorical_vars <- c()
openEnded_vars <- c()

for (colname in names(food)) {
  column <- food[[colname]] # grab the column itself
  
  if (is.numeric(column)) {
    # store numeric columns
    numerical_vars <- c(numerical_vars, colname)
  
  } else if (is.factor(column)) {
    nlev <- nlevels(column)
    
    if(nlev <= 12) { # a bit higher than for char columns
      # store categorical columns
      categorical_vars <- c(categorical_vars, colname)
    } else {
        # store open response columns
        openEnded_vars <- c(openEnded_vars, colname)
    }
  
  } else if (is.character(column)) {
    # heuristic: check number of unique entries
    nunique <- length(unique(column))
    
    if (nunique <= 8) { #slightly lower threshold
      categorical_vars <- c(categorical_vars, colname)
    } else {
        openEnded_vars <- c(openEnded_vars, colname)
    }
  }
}
```

<!-- Utility 1 -->

<!-- Util. 1 (1): View sorted dataframe -->

```{r, echo=FALSE}
# Create a flat list of of column order by variable category class
combined_vars = c(numerical_vars, categorical_vars, openEnded_vars)
flat_combined_vars <- unlist(combined_vars)

# Sort 'food' dataframe variables by categorization class and view it.
food_sorted <- food[, flat_combined_vars]
View(food_sorted)
```

<!-- Utility 2 -->

<!-- Util. 2 (1): View subset dataframe for categorized variables-->

```{r, echo=FALSE}
# Subset the 'food' dataframe for numerical variables
numerical <- food[, numerical_vars]
View(numerical)
```

<!-- Util. 2 (2): View subset dataframe for categorized variables-->

```{r, echo=FALSE}
# Subset the 'food' dataframe for categorical variables
categorical <- food[, categorical_vars]
View(categorical)
```

<!-- Util. 2 (3): View subset dataframe for categorized variables -->

```{r, echo=FALSE}
# Subset the 'food' dataframe for open ended variables
openEnded <- food[, openEnded_vars]
View(openEnded)
```

Next, we create two distinct dataframes for deeper insights into the data and to generate questions worth analysing.

```{r Expl. - Create Dataframes}
# Variables (categorized) as lists
x_vars <- numerical_vars
g_vars <- categorical_vars
o_vars <- openEnded_vars

# List lengths
x_len <- length(x_vars)
g_len <- length(g_vars)
o_len <- length(o_vars)

# Longest list as value
max_len <- max(x_len, g_len, o_len) 

# Pad shorter two lists to match the longest list
x_vars_padded <- c(x_vars, rep("", max_len - x_len))
g_vars_padded <- c(g_vars, rep("", max_len - g_len))
o_vars_padded <- c(o_vars, rep("", max_len - o_len))

# Make a dataframe of the categorized variables (x, g, o)
AllVars_xgo <- data.frame(
  'x (numerical)' = x_vars_padded,
  'g (categorical)' = g_vars_padded,
  'o (open ended)' = o_vars_padded 
)




# All possible combinations of variables (x, g)
AllCombos_xg <- expand.grid(
  x = x_vars, 
  g = g_vars, 
  stringsAsFactors = FALSE
) %>% 
  rename(
    `x (numerical)` = x,
    `g (categorical)` = g
  )
```

Note, this function helps reduce code duplication.

<!-- Function: construct and show datatable -->
```{r}
construct_and_show_datatable <- function(df) {
  # Mix hardcoded with calculated values for datatable > options > lengthMenu
  all <- as.numeric(nrow(df))
  options <- c(5, 30, all)
  
  # Display table
  datatable(
    df,
    options = list(
      lengthMenu = options,
      scrollX = TRUE,
      fixedColums = TRUE,
      columnDefs = list(
        list(className = 'dt-left', targets = 0)
      )
    )
  )
}
```

#### Hide

*Click the buttons to view either table.*

----

</BR>

#### Categories

**Categories** include *x*, *g*, and *o* (numerical/categorical/open ended).

```{r}
construct_and_show_datatable(AllVars_xgo)
```

----

</BR>

#### Combinations

Variable **combinations** are pairings of only *x* and *g* (numerical/categorical) variables ordered by the *g* (categorical) variable.

```{r}
construct_and_show_datatable(AllCombos_xg)
```

----

</BR>

```{=html}
<!--
.
.
.
.
.
.
-->
```

```{=html}
<!-------------------------------------------------------
.                          ANALYSIS                    
-------------------------------------------------------->
```

## Analysis

> **Question:** Does ones' perception of their weight impact their GPA? </BR> **Hypotheses:** We will rely on the standard hypotheses for a Kruskal-Wallis Rank Sum test. </BR> $$
> H_0: \text{All samples are from the same distribution.}
> \\
> H_a: \text{At least one sample's distribution is stochastically different.}
> $$ **Conclusion:** One's perception of their weight does not significantly bear on the resulting GPA.

First, the data are cleaned for plotting and testing. In this case, proper levels are defined for values that have no inherent order such as is the case for the ranking levels found in the `self_perception_weight` variable.

<!-- Clean data for plotting and testing -->

```{r Anly_1 - Clean, warning=FALSE}
food1 <- food %>%
  # convert GPA to numeric and store in a temp column
  mutate(GPA_num = as.numeric(as.character(GPA))) %>%
  # filter to only rows with valid GPA
  filter(!is.na(GPA_num)) %>%
  # filter on self-perception
  filter(self_perception_weight %in% 1:6) %>%
  # convert GPA to numeric
  mutate(GPA = GPA_num) %>% 
  # drop levels
  droplevels() %>%
  # convert to a factor with six levels
  mutate(self_perception_weight = factor(self_perception_weight, levels = 1:6)) %>% 
  # remove the temp column and select the needed columns
  select(GPA, self_perception_weight, -GPA_num)
```

<!-- Utility 1 -->

<!-- Util. 1 (1) -->

```{r, echo=FALSE, include=FALSE}
View(food1)
```

<!-- Utility 2 -->

<!-- Util. 2 (1) -->

```{r, echo=FALSE, include=FALSE}
str(food1)
```

### Plots {.tabset .tabset-pills .tabset-fade}

Second, two selected variables—`Self Perception of Weight` (6 levels) and `GPA`— are plotted side-by-side to visually reveal if any stochastic variance stands out.

<u>Legend: **Self Perception of Weight**</u>

-   6 - I dont think myself in these terms
-   5 - Overweight
-   4 - Slightly overweight
-   3 - Just right
-   2 - Very fit
-   1 - Slim

#### Boxplot

This plot shows min, Q1, median, Q3, and max for each factor level.

Ovservably, no one factor level stands out as having significant stochastic variance from the other factor levels.

<!-- Boxplot of variables -->

```{r}
# Does one's self-perception of their weight impact their GPA?
food1 %>% 
boxplot(
  GPA ~ self_perception_weight, 
  data = ., 
  xlab = "Self Perception of Weight", 
  main = "Regardless of Self Perception of Weight GPA Varies Little"
)
```

#### Dotplot

This plot allows for a clearer presentation of data congregation, or where data is heavily grouped for each factor level.

```{r}
# Does one's self-perception of their weight impact their GPA?
dotplot(
  GPA ~ self_perception_weight, 
  data = food1, 
  xlab = "Self Perception of Weight", 
  main = "Regardless of Self Perception of Weight GPA Varies Little",
  na.rm = TRUE
)
```

</BR>

### Statistical Test

Next, perform a **Kruskal-Wallis Rank Sum** test to assess statistical significance and display the results. Per usual, let α = 0.05 for a 95% confidence level.

<!-- Raw output: -->

<!-- Kruskal-Wallis Rank Sum Test -->

```{r}
# Test the realtionship between GPA and self_perception_weight
test_result <-food1 %>% 
  kruskal.test(GPA ~ self_perception_weight, data = .)

# test_result # Raw output
```

<!-- Neat output: -->

<!-- Display the results -->

```{r}
pander(test_result) # Neat output
```

It is shown in these results that the **correlation between the selected variables is low** (p-values: 0.6216). Otherwise, the resulting `GPA` values would be shown to come from significantly different distributions identifiably separate one from another by one's `Self Perception of Weight`. However, from this test it is clear this is not likely to be the case, or in other words one's self-perception of their own weight has little impact on their GPA.

### Numerical Summary

Lastly, a numerical summary of GPA for each of the factor levels will further confirm the similarlity in GPA scores even when sorted by factor level (`self_perception_weight`).

```{r}
food2 <- food1 %>% 
  rename(SPW = self_perception_weight)

fvs <- food2 %>% 
  favstats(GPA ~ SPW, data = .) #%>%
  # select(SPW, median) %>%
  

pander(fvs)
```

Interestingly, when we <u>revise the above boxplots</u> showing key values traced across the factor levels (SPW) with lines it becomes more clear how the outliers of each level skew the mean but not the median compared to other factor levels further confirming our choice of statistical test which uses the median instead of the mean to be the more appropriate option.

<!-- Revised Boxplot (with lines) -->
```{r}
ggplot() +
  # boxplots
  geom_boxplot(
    data = food2,
    aes(x = SPW, y = GPA),
    fill = "lightgray",
    color = "black"
  ) +
  # summary lines
  geom_line(
    data = fvs,
    aes(x = SPW, y = min, group = 1, color = "Min"),
    alpha = .8, linetype = "dotted"
  ) +
  geom_line(
    data = fvs,
    aes(x = SPW, y = max, group = 1, color = "Max"),
    alpha = .8, linetype = "dotted"
  ) +
  geom_line(
    data = fvs,
    aes(x = SPW, y = median, group = 1, color = "Median")
  ) +
  geom_line(
    data = fvs,
    aes(x = SPW, y = mean, group = 1, color = "Mean")
  ) +
  scale_x_discrete(
    labels = levels(food2$SPW)
  ) +
  labs(
    title = "Summary GPA across SPW levels with Boxplots",
    x = "Self Perception Weight",
    y = "GPA",
    color = "Statistic"
  ) +
  theme_classic()
```

We could even look at the entire numerical summary as a <u>dotplot</u> with only the **median** and **mean** plotted with lines of course showing the same trend as above: when outliers pull on the shape of the distribution the mean follows whereas the median does so far less making it the better anchor for estimates reliant on a measure of the tendency towards centrality.

<!-- Dotplot (with lines) -->
```{r}
# This is what the `tidyr` library was imported for.

# Elongate the data
fvs_long <- fvs %>%
  pivot_longer(cols = c(min, Q1, median, Q3, max, mean, sd),
               names_to = "Statistic",
               values_to = "Value")
# Define the negative offset for use in straightening labels and whatnot.
neg_offset <- -.2

# Primary plot
ggplot(fvs_long, aes(x = as.numeric(SPW) + neg_offset, y = Value, color = Statistic)) +
  # Dotplot: jittered value distribution
  geom_dotplot(data = food2, aes(x = SPW, y = GPA), 
               binaxis = "y", stackdir = "up", fill = "gray90", color = "gray90", binwidth = 0.05, alpha = 0.3
  ) +
  # Dotplot: summary stats
  geom_point(size = 2, position = position_dodge(width = 0.0), shape = 1) +
  # Line charts: summary stats
  geom_line(data = fvs, aes(x = as.numeric(SPW) + neg_offset, y = median, color = "median")) + # Line to track the median
  geom_line(data = fvs, aes(x = as.numeric(SPW) + neg_offset, y = mean, color = "mean")) + # Line to track the mean
  # Data labels: select summary stats
  geom_label( # label: median
    data = fvs,
    aes(x = as.numeric(SPW) - .5, y = median + .2, label = round(median, digits=2), color = "median"),
    size = 2.2,
    fill = "gray100",
    show.legend = FALSE
  ) +
  geom_label( # label: mean
    data = fvs,
    aes(x = as.numeric(SPW) - .5, y = mean - .2, label = round(mean, digits=2), color = "mean"),
    size = 2.2,
    fill = "gray100",
    show.legend = FALSE
  ) +
  # Vertical line: Separate the factor levels
  geom_vline(xintercept = seq(1, 6, by = 1) + 0.4, color = "gray80", linetype = "dashed") +
  labs(
    title = "Summary Statistics of GPA by Self-Perception of Weight",
    x = "Self Perception of Weight",
    y = "GPA"
  ) +
  theme_classic()
```

</BR>

### Conclusion


<!-- ************************************************
NOTE: THE CONCLUSION GOT VERY BLOATED. SO I REDID IT.
************************************************ -->

<!-- 
——————————————————————————
CONCLUSION V.1 (bloated)
——————————————————————————
-->

<!-- Comment out V1 -->

<!-- NOTE: 
In order to comment out this section which is a combination of R code chunks and markdown I commented out each line (even if it was a comment already) using html comments then wrapped the block in an R code chunk and again commented out the block with R style comments. This works because it does and I don't want to explain myself. I did what I needed to do haha. -->

```{r, echo=FALSE, include=FALSE, warning=FALSE, message=FALSE}
# <!-- <!-- Differences (median - mean) --> -->
# <!-- ```{r} -->
# <!-- # Find the differences between the median and mean for each factor level -->
# <!-- fvs2 <- fvs %>% -->
# <!--   mutate( -->
# <!--     diff = round(median - mean, digits=3) # I renamed this variable below. I want to call it diff here for simplicity sake. -->
# <!--   ) -->
# 
# <!-- # Construct a pivoted table of the differences -->
# <!-- fvs2_wide <- fvs2 %>% -->
# <!--   select(SPW, 'median - mean' = diff) %>% -->
# <!--   mutate(Calculation = 'median - mean') %>% -->
# <!--   pivot_wider( -->
# <!--     names_from = SPW, -->
# <!--     values_from = 'median - mean') %>% -->
# <!--   relocate(Calculation, .before = everything()) -->
# 
# 
# <!-- # Calculate the average difference -->
# <!-- avg_diff <- round(mean(fvs2$diff), digits = 2) -->
# <!-- ``` -->
# 
# <!-- As previously stated both at the beginning and throughout, the two selected variables—`self_perception_weight` (also called `SPW`) and `GPA`—have little to do with each other. Therefore, a change in one will likely correspond with very insignificant change in the other. -->
# 
# <!-- Below we can see for each factor level the <u>calculated difference between the median and mean</u> which clearly demonstrates the overall trend that the `median` is __higher__ than the `mean` in this case, or less impacted by outliers dragging the data and potentially left skewing the distribution. -->
# 
# <!-- ```{r} -->
# <!-- # Display long, subsetted table -->
# <!-- pander(fvs2_wide) -->
# <!-- ``` -->
# 
# <!-- Also, the average difference (`median - mean`) is `r avg_diff`. -->
# 
# <!-- ```{r} -->
# <!-- # Calculate the difference -->
# <!-- mean_of_means <- mean(fvs$mean) # Mean of means -->
# <!-- mean_of_means_rounded <- round(mean(fvs$mean), digits=2) # Mean of means rounded -->
# <!-- mean_lvl5 <- fvs$mean[fvs$SPW == 5] # Mean (factor lvl 5) -->
# <!-- mean_lvl5_rounded <- round(fvs$mean[fvs$SPW == 5], digits = 2) # Mean (factor lvl 5) -->
# 
# <!-- diff_lvl5 <- mean_of_means - mean_lvl5 -->
# <!-- diff_lvl5_rounded <- round(mean_of_means - mean_lvl5, digits=2) # Difference -->
# 
# <!-- red <- "firebrick" -->
# 
# <!-- # --- -->
# 
# <!-- mean_lvl6 <- fvs$mean[fvs$SPW == 6] # Mean (factor lvl 6) -->
# <!-- mean_lvl6_rounded <- round(fvs$mean[fvs$SPW == 6], digits = 2) # Mean (factor lvl 6) -->
# 
# <!-- diff_lvl6 <- mean_of_means - mean_lvl6 -->
# <!-- diff_lvl6_rounded <- round(mean_of_means - mean_lvl6, digits=2) -->
# 
# <!-- if (diff_lvl6 >= 0){ -->
# <!--   above_or_below <- "above" -->
# <!--   txt_color <- "forestgreen" -->
# <!-- } else { -->
# <!--   above_or_below <- "below" -->
# <!--   txt_color <- "firebrick" -->
# <!--   } -->
# <!-- ``` -->
# 
# <!-- <!-- Summary of paragraph: include numerical references to mean values and compare/contrast --> -->
# <!-- The lowest `mean` value is for factor __level 5__ (Overweight) at __`r mean_lvl5_rounded`__ (rounded) which is -->
# <!-- <!-- Apply styling --> -->
# <!-- <span style ="font-weight: bold; color: `r red`;"> lower</span> -->
# <!-- than the mean of means (`r mean_of_means_rounded`) by __`r diff_lvl5_rounded`__. This is especially usefully for why this data is hard to analyze when the `mean` is all over the place. Notably, immediately after dropping so low the `mean` shoots back up to __`r mean_lvl6_rounded`__ (rounded) for factor __level 6__ which is __`r diff_lvl6_rounded`__ -->
# <!-- <!-- Apply styling --> -->
# <!-- <span style ="font-weight: bold; color: `r txt_color`;">`r above_or_below`</span> -->
# <!-- the mean of means. -->
# 
# <!-- Further research could consider different pairings of variables and perhaps use the results of this analysis to determine which variables a more logically connected and potentially impactful on one another. -->
```


<!-- 
——————————————————————————
CONCLUSION V.2 (debloated)
—————————————————————————— 
-->

As discussed throughout this analysis, there appears to be __little meaningful relationship__ between a person's self-perception of their weight (`self_perception_weight`, or `SPW`) and their GPA (`GPA`). This is confirmed when examining the __median GPA values__ across SPW levels, which remain relatively stable.

```{r}
# Max & Min 'median' (SPW levels: all)
medians <- fvs$median
min_median <- min(medians) # Lvl 6
max_median <- max(medians) # Lvl 1, 2, 4

# Median of medians (SPW levels: all)
median_of_medians <- median(medians)

# Calculate difference
diff_min <- median_of_medians - min_median
diff_max <- median_of_medians - max_median

```

For example, the median GPAs for all levels range narrowly from `r min_median` to `r max_median`. Even the __lowest median__, found at SPW level 6 (I don't think myself in these terms), is __`r min_median`__, only __`r diff_min` points below__ the overall median of medians (`r median_of_medians`). The highest median, which is the same for SPW levels 1 (Slim), 2 (Very fit), and 4 (Slightly overweight), is __`r max_median`__, only __`r diff_max` points above the overall median of medians.

This minimal fluctuation suggests that __self-perceived weight (SPW) does not significantly influence academic performance (GPA)__ in this dataset.

Further analysis could explore other variable pairings that may be more strongly correlated, or dive deeper into subgroups to assess if specific intersections (e.g., `gender` + `SPW`) reveal more nuance.

</BR>

<!-- Add two new lines at the end of the analysis for better output display and user experience. -->

</BR></BR>
